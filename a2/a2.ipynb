{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## CPSC 533V: Assignment 2 - Tabular Q Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p style=\"font-size: 1.2em;\">Due Date: Wed Oct 6, 2021</p>\n",
    "<p style=\"font-size: 1.2em;\">100 Points Total (9% of final grade)</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important Notes\n",
    "\n",
    "* Your deliverable is this Jupyter Notebook. Submission will be done via Canvas.\n",
    "* For instructions on installing and running Jupyter Notebook: https://jupyter.org/install\n",
    "    * Start by cloning this repository: git clone git@github.com:UBCMOCCA/CPSC533V_2021W1.git\n",
    "    * Install Jupyter Notebook using either `conda install jupyter` or `pip install jupyter`\n",
    "    * Inside the `a2` folder, run `jupyter notebook` and a webpage should open in the browser\n",
    "    * If not, follow the instruction in terminal to launch an interactive session\n",
    "* If you use additional Python packages, please list them  as it will help with grading. \n",
    "* **We recommend working in groups of two**. List your names and student numbers below (if you use a different name on Canvas).\n",
    "\n",
    "<ul style=\"list-style-type: none; font-size: 1.2em;\">\n",
    "<li>Name (and student ID):Haomiao Zhang 33074155</li>\n",
    "<li>Name (and student ID):</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Debugging Tips\n",
    "\n",
    "* Debugging in Jupyter Notebook can be using `pdb` or `ipdb`\n",
    "* Insert `import ipdb; ipdb.set_trace()` to where you want to set a breakpoint\n",
    "* See https://docs.python.org/3/library/pdb.html#debugger-commands for useful commands\n",
    "* Remember to quit out from an `ipdb` session, otherwise you may wonder why a code cell is taking forever to complete ðŸ˜‰"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "##  Tabular Q-Learning\n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{max}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In this assignment you will implement tabular Q-learning and apply it to CartPole â€“ an environment with a **continuous** state space.  To apply the tabular method, **you will need to discretize the CartPole state space** by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Assignment goals:**\n",
    "- To become familiar with Python, NumPy, and OpenAI Gym\n",
    "- To understand and implement tabular Q-learning\n",
    "- To experiment tabular Q-learning on your implemention of discrete CartPole environment\n",
    "- (Optional) To develop further intuition regarding possible variations of the algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], , [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ðŸ˜‰).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning, policy gradient, and search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Install dependencies\n",
    "# Only run if necessary\n",
    "!pip install gym\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gym in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from gym) (1.20.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (1.20.1)\n",
      "Requirement already satisfied: matplotlib in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /home/haomiaoz/anaconda3/envs/openaigym/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 1. Explore the CartPole environment [18 pts]\n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment]( https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "Read this brief introduction on [OpenAI Gym](https://gym.openai.com/docs/) to get started. \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = gym.make('CartPole-v1')  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "cartpole = env.unwrapped\n",
    "cartpole?"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0;31mType:\u001b[0m        CartPoleEnv\n",
      "\u001b[0;31mString form:\u001b[0m <CartPoleEnv<CartPole-v1>>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/openaigym/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Description:\n",
      "    A pole is attached by an un-actuated joint to a cart, which moves along\n",
      "    a frictionless track. The pendulum starts upright, and the goal is to\n",
      "    prevent it from falling over by increasing and reducing the cart's\n",
      "    velocity.\n",
      "\n",
      "Source:\n",
      "    This environment corresponds to the version of the cart-pole problem\n",
      "    described by Barto, Sutton, and Anderson\n",
      "\n",
      "Observation:\n",
      "    Type: Box(4)\n",
      "    Num     Observation               Min                     Max\n",
      "    0       Cart Position             -4.8                    4.8\n",
      "    1       Cart Velocity             -Inf                    Inf\n",
      "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
      "    3       Pole Angular Velocity     -Inf                    Inf\n",
      "\n",
      "Actions:\n",
      "    Type: Discrete(2)\n",
      "    Num   Action\n",
      "    0     Push cart to the left\n",
      "    1     Push cart to the right\n",
      "\n",
      "    Note: The amount the velocity that is reduced or increased is not\n",
      "    fixed; it depends on the angle the pole is pointing. This is because\n",
      "    the center of gravity of the pole increases the amount of energy needed\n",
      "    to move the cart underneath it\n",
      "\n",
      "Reward:\n",
      "    Reward is 1 for every step taken, including the termination step\n",
      "\n",
      "Starting State:\n",
      "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
      "\n",
      "Episode Termination:\n",
      "    Pole Angle is more than 12 degrees.\n",
      "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
      "    the display).\n",
      "    Episode length is greater than 200.\n",
      "    Solved Requirements:\n",
      "    Considered solved when the average return is greater than or equal to\n",
      "    195.0 over 100 consecutive trials.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#snippet\n",
    "obs =env.reset()\n",
    "print(obs)\n",
    "print(obs[2])\n",
    "print(env.action_space)\n",
    "action = env.action_space.sample()\n",
    "print(action)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.03512149  0.0268827   0.00301548 -0.00155008]\n",
      "0.003015485\n",
      "Discrete(2)\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.1 [10pts] Complete the `TODO`s in the next code block**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Q1.1\n",
    "\n",
    "# Runs a single episode and render it\n",
    "# Try running this before editing anything\n",
    "\n",
    "obs = env.reset()  # get initial state/observation\n",
    "reward_sum =0\n",
    "\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs` \n",
    "    \n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    if obs[2]<=0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "    #action = env.action_space.sample()  # random action\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    reward_sum+=reward\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "# TODO: print out your total sum of rewards here\n",
    "print(reward_sum)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "55.0\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.2. [2pts] Describe the observation and action spaces of CartPole.  What does each of the values mean/do?**\n",
    "\n",
    "*Hint: Look at the full [source code here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) if you haven't already.*\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The observation space has 4 dimensions, which represent the linear position and velocity of the cart and the angular position and velocity of the pole."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.3. [2pts] What distribution is used to sample initial states? (see the `reset` function)** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A uniform distribution between -0.05 and 0.05 for each state variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.4. [2pts] What is the termination condition, which determines if the `env` is `done`?** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are multiple:\n",
    "* Pole angle is larger than 12 degrees in either directions\n",
    "* Cart reaches the edge, which is the position of cart is larger than 2.4 in either direction\n",
    "* Episode length is longer than 200"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1.5. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When pole is falling to the left, push the cart to the left continously. If the pole is falling to the right, push the cart to the right continously. Only the angular position of the pole is used. The maximum reward during trail is 61, and the average is around 45. Random policy would achieve a reward of 20 on average, so the new policy is better than random policy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2. Discretize the environment [32 pts]\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer questions 2.3 and 2.4**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.1 [15pts for passing test_normed]** Normalize observation space"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Q2.1\n",
    "\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    # TODO: implement this function\n",
    "    raise NotImplementedError('TODO')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs, _, done, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if done: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.2 [13pts for passing test_binned]**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Q2.2\n",
    "\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "    raise NotImplementedError('TODO')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### TEST 2.2\n",
    "obs = env.reset()\n",
    "env.close()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int, \"You should also make sure to cast your answer to int using np.int() or arr.astype(np.int)\" \n",
    "    \n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3. [2pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states)?**\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: answer here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.4. [2pts] After discretizing action space, is the dynamics deterministic or non-deterministic? Explain your answer in one to two sentences.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: answer here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 3. Solve the env [30 pts] \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 10\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.1 [25pts] Implement Q-learning**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Q3.1\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3.2 [5pts] Plot the learning curve**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Q3.2\n",
    "\n",
    "# TODO: Plot the learning curve.\n",
    "#       Below is a snippet for generate a curve with upper and lower bounds.\n",
    "#       From your training loop above, save the episode rewards.\n",
    "#       Rerun the training code a few times to get min and max.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(100)\n",
    "y = x ** 2\n",
    "plt.plot(y)\n",
    "\n",
    "lower_bound = 0.5*y\n",
    "upper_bound = 2.0*y\n",
    "plt.fill_between(x, lower_bound, upper_bound, alpha=0.5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7f5e34ad8730>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Experiments [20 pts]\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4.2. [10pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: answer here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: answer here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4.3. [10pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: answer here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the experiment and report the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: answer here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (optional)\n",
    "\n",
    "- does the learning rate make a difference?\n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next homework that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('openaigym': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "cce4f570f01464b63e2fe4e09d261a885461b6def4c80158439656d16c794d53"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}